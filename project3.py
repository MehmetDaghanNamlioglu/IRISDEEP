# -*- coding: utf-8 -*-
"""proj3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BgXIXHrbRRnMJ8KebpVSGAILyQzlw2ke
"""



from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization
from keras.preprocessing.image import load_img, save_img, img_to_array
from keras.applications.imagenet_utils import preprocess_input
from keras.models import Model, Sequential
from keras.preprocessing import image
import tensorflow as tf

from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import glob
from os import path
import numpy as np
from PIL import Image, ImageDraw


def imageprocess(img,x,y,rad):
    return np.asarray(img.crop((y-rad,x-rad,y+rad,x+rad)).resize((256,256),1).convert('L')).astype(int)

def makeimagelist(path = "/content/drive/MyDrive/Database"):

    f = open(path+"/Parameters.txt","r")
    arr = f.read()
    #print(arr)
    arr = arr.split("\n")[2:][:-1]
    imarr = []
    targetarr = []
    for i in range(len(arr)):
        print(i)
        arr[i] = np.asarray(arr[i].split(","))
        temp = arr[i][0].split("_")
        if ('l' in temp[3].split("-")[2]):
            targetarr.append((int(temp[0][1:])*2)-1)
        else:
            targetarr.append(int(temp[0][1:])*2)
        image=Image.open(path +"/"+ arr[i][0]).convert('L')
        image = imageprocess(image,int(arr[i][1]),int(arr[i][2]),int(arr[i][3]))
        imarr.append(image)
    targetarr = np.asarray(targetarr)
    t1=np.asarray(arr)
    t2=np.asarray(imarr)
    t3=np.asarray(targetarr)
    return t1,t2,t3

t1,t2,t3 = makeimagelist()

import tensorflow.keras.utils as tfut
from sklearn.model_selection import train_test_split

normalized = []
for i in range(len(t2)):
  normalized.append((t2[i].astype(float))/255)
normalized = np.asarray(normalized)
onehot = tfut.to_categorical(t3-1, 420)
data = np.expand_dims(normalized, axis=-1)


Xtr, Xtst, ytr, ytst = train_test_split(data, onehot, test_size=0.25)

s1, s3, s2, s4 = train_test_split(data, onehot, test_size=0.5)
s3, s5, s4, s6 = train_test_split(s3, s4, test_size=0.5)

#@title

def plot_hist(hist):
    plt.plot(hist.history["accuracy"])
    plt.plot(hist.history["val_accuracy"])
    plt.title("model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.show()

#Basic model
model = Sequential()
model.add(Convolution2D(2, (5, 5), activation='relu', input_shape=(256,256,1)))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Convolution2D(4, (5, 5), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(420))
model.add(Activation("softmax"))

opt = tf.keras.optimizers.RMSprop(learning_rate = 0.0005)
model.compile(loss="categorical_crossentropy", optimizer= opt, metrics= ['accuracy'] )
b_size = 8
e_value = 15
model.summary()

hist = model.fit( s1 , s2 ,batch_size=b_size,validation_data=(s3,s4), epochs= e_value)
plot_hist(hist)

results = model.evaluate(s5, s6, batch_size=4)
print("test loss, test acc:", results)

from keras.regularizers import l2

#Complex model

model = Sequential()
model.add(Convolution2D(2, (5, 5), activation='relu', input_shape=(256,256,1)))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Dropout(0.15))
model.add(Convolution2D(4, (5, 5),kernel_regularizer=l2(0.01), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Dropout(0.6))
model.add(Convolution2D(8, (5, 5), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Dropout(0.85))
model.add(Flatten())
model.add(Dense(420))
model.add(Activation("softmax"))

opt = tf.keras.optimizers.RMSprop(learning_rate = 0.001)
model.compile(loss="categorical_crossentropy", optimizer= opt, metrics= ['accuracy'] )
b_size = 8
e_value = 400
model.summary()

hist = model.fit( Xtr , ytr ,batch_size=b_size,validation_data=(Xtst,ytst), epochs= e_value)
plot_hist(hist)

results = model.evaluate(s5, s6, batch_size=4)
print("test loss, test acc:", results)

from google.colab import drive
drive.mount('/content/drive')